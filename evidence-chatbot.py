# ì—¬ê¸°ì„œë¶€í„° ì„¸ì¤„ì€ ë¡œì»¬í™˜ê²½ì—ì„œ ëŒë¦´ ë•Œì—ëŠ”(ì¦‰ ì›¹ì‚¬ì´íŠ¸ë¡œ ë°°í¬ ì•ˆí•˜ê³  ê·¸ëƒ¥ í„°ë¯¸ë„ì—ì„œ ëŒë¦´ë•Œ) ì£¼ì„ì²˜ë¦¬ í•´ì£¼ì…”ì•¼í•©ë‹ˆë‹¤. 
# ë°°í¬í• ë•Œì—ëŠ” ì£¼ì„ì²˜ë¦¬í•˜ì‹œë©´ ì•ˆë©ë‹ˆë‹¤. 
# ì£¼ì„ì²˜ë¦¬ ë°©ë²•ì€ "Ctrl + "/"" ëˆ„ë¥´ê¸°
# ---------------------------------------------------
# __import__('pysqlite3')
# import sys
# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ---------------------------------------------------

import os
import tempfile
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import StreamlitChatMessageHistory
from langchain.embeddings import OpenAIEmbeddings
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma

st.set_page_config(page_title="ìë£Œ ê²€ì¦ ì±—ë´‡", page_icon="ğŸ‘¾")
st.title("ìë£Œ ê²€ì¦ ì±—ë´‡")

# ë¡œì»¬ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•´ë³¼ë•Œ
os.environ["OPENAI_API_KEY"] ="sk-blablabla"

# Streamlit ë°°í¬í• ë•Œ
# Streamlit ì•±ì˜ í™˜ê²½ì„¤ì •ì—ì„œ ê¼­ OPENAI_API_KEY = "sk-blabalabla"ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”!
# os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# AIê°€ ìƒì„±í•œ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ 
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = ""):
        self.container = container
        self.text = initial_text
        self.run_id_ignore_token = None

    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):
        # Workaround to prevent showing the rephrased question as output
        if prompts[0].startswith("Human"):
            self.run_id_ignore_token = kwargs.get("run_id")

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        if self.run_id_ignore_token == kwargs.get("run_id", False):
            return
        self.text += token
        self.container.markdown(self.text)

# í•™ìŠµì‹œí‚¨ ìë£Œì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•´ retrieverì—ì„œ ë¶ˆëŸ¬ì˜¨ ë‚´ìš©ì„ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ (ìë£Œ ê²€ì¦)
class PrintRetrievalHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.status = container.status("**Context Retrieval**")

    def on_retriever_start(self, serialized: dict, query: str, **kwargs):
        self.status.write(f"**Question:** {query}")
        self.status.update(label=f"**Context Retrieval:** {query}")

    def on_retriever_end(self, documents, **kwargs):
        for idx, doc in enumerate(documents):
            source = os.path.basename(doc.metadata["source"])
            self.status.write(f"**Document {idx} from {source}**")
            self.status.markdown(doc.page_content)
        self.status.update(state="complete")


# ì–´ë–¤ íŒŒì¼ì„ í•™ìŠµì‹œí‚¤ëŠ”ì§€ì— ë”°ë¼ ì½”ë“œë¥¼ ë°”ê¿”ì£¼ì„¸ìš”. ex) pdf, html, csv

# ì²«ë²ˆì§¸ êµ¬í˜„ ë°©ë²•: ì›¹ì‚¬ì´íŠ¸ url í•™ìŠµì‹œí‚¤ê¸°
# ---------------------------------------------------
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://dalpha.so/ko/howtouse?scrollTo=custom")
data = loader.load()
# ---------------------------------------------------


# ë‘ë²ˆì§¸ êµ¬í˜„ ë°©ë²•: pdf í•™ìŠµì‹œí‚¤ê¸°
# ë¨¼ì € VSCodeì—ì„œ ë§Œë“  ì´ í´ë” ë‚´ì— pdf íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì…”ì•¼í•´ìš”!
# ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ë¶€ë¶„ì˜ ì½”ë“œ ì£¼ì„ì„ ì—†ì• ì£¼ì„¸ìš”
# ---------------------------------------------------
# from langchain.document_loaders import PyPDFLoader

# loader = PyPDFLoader("íŒŒì¼ì´ë¦„.pdf")
# pages = loader.load_and_split()

# data = []
# for content in pages:
#     data.append(content)
# ---------------------------------------------------


# ì„¸ë²ˆì§¸ êµ¬í˜„ ë°©ë²•: csv í•™ìŠµì‹œí‚¤ê¸°
# ë¨¼ì € VSCodeì—ì„œ ë§Œë“  ì´ í´ë” ë‚´ì— csv íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì…”ì•¼í•´ìš”!
# ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ë¶€ë¶„ì˜ ì½”ë“œ ì£¼ì„ì„ ì—†ì• ì£¼ì„¸ìš”
# ---------------------------------------------------
# from langchain.document_loaders.csv_loader import CSVLoader

# loader = CSVLoader(file_path='íŒŒì¼ì´ë¦„.csv')
# data = loader.load()
# ---------------------------------------------------

# ì˜¬ë¦° íŒŒì¼ ë‚´ìš© ìª¼ê°œê¸°
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
all_splits = text_splitter.split_documents(data)

# ìª¼ê°  ë‚´ìš© vectorstore ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œí•˜ê¸°
vectordb = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())

# ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œ í•œ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ì…‹ì—…
retriever = vectordb.as_retriever(search_type="mmr", search_kwargs={"k": 2, "fetch_k": 4})

# ëŒ€í™” ë‚´ìš© ê¸°ë¡í•˜ëŠ” ë©”ëª¨ë¦¬ ë³€ìˆ˜ ì…‹ì—…
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=msgs, return_messages=True)

# Setup LLM and QA chain
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo", temperature=0, streaming=True
)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm, retriever=retriever, memory=memory, verbose=True
)

if len(msgs.messages) == 0 or st.sidebar.button("ë©”ì„¸ì§€ ê¸°ë¡ ì‚­ì œí•˜ê¸°"):
    msgs.clear()
    msgs.add_ai_message("ì•ˆë…•í•˜ì„¸ìš”. ì„œë¹„ìŠ¤ ë§¤ë‰´ì–¼ ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹œë©´ ë¬¼ì–´ë´ì£¼ì„¸ìš”.")

avatars = {"human": "user", "ai": "assistant"}
for msg in msgs.messages:
    st.chat_message(avatars[msg.type]).write(msg.content)

if user_query := st.chat_input(placeholder="Dalpha AI storeëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?"):
    st.chat_message("user").write(user_query)

    # AIì˜ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ìƒì„±í•œ ë‹µë³€ì— ëŒ€í•œ ì¦ê±° ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        retrieval_handler = PrintRetrievalHandler(st.container())
        stream_handler = StreamHandler(st.empty())
        response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])